{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data, Define, Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "class MalwareGraphDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_dict, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset to load labeled graph data from multiple folders.\n",
    "        \n",
    "        Parameters:\n",
    "        - root_dir: str, root directory containing subdirectories for each label.\n",
    "        - label_dict: dict, mapping of subdirectory names to numeric labels.\n",
    "        \"\"\"\n",
    "        super(MalwareGraphDataset, self).__init__(root=root_dir, transform=transform, pre_transform=pre_transform)\n",
    "        self.root_dir = root_dir\n",
    "        self.label_dict = label_dict\n",
    "        self.graph_files = []\n",
    "        \n",
    "        # Collect all .pt files with assigned labels\n",
    "        for folder_name, label in label_dict.items():\n",
    "            folder_path = os.path.join(root_dir, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for file_name in os.listdir(folder_path):\n",
    "                    if file_name.endswith(\".pt\"):\n",
    "                        self.graph_files.append((os.path.join(folder_path, file_name), label))\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.graph_files)\n",
    "\n",
    "    def get(self, idx):\n",
    "        # Load graph data and assign label\n",
    "        file_path, label = self.graph_files[idx]\n",
    "        data = torch.load(file_path)\n",
    "        data.y = torch.tensor([label], dtype=torch.long)  # Set the label\n",
    "        return data\n",
    "\n",
    "    def count_samples_per_class(self):\n",
    "        \"\"\"\n",
    "        Counts the number of samples per class based on the label dictionary.\n",
    "        \"\"\"\n",
    "        class_counts = defaultdict(int)\n",
    "        for _, label in self.graph_files:\n",
    "            class_counts[label] += 1\n",
    "\n",
    "        # Print the count for each class\n",
    "        print(\"Number of data points per class:\")\n",
    "        for label, count in class_counts.items():\n",
    "            class_name = list(self.label_dict.keys())[list(self.label_dict.values()).index(label)]\n",
    "            print(f\"{class_name} (Label {label}): {count} samples\")\n",
    "\n",
    "# Define the label mapping based on folder names\n",
    "label_dict = {\n",
    "    \"Benign\": 0,\n",
    "    \"Adware\": 1,\n",
    "    \"Banking\": 2,\n",
    "    \"Riskware\": 3,\n",
    "    \"Smsware\": 4\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "root_directory = \"/home/belief/Desktop/MalwareDetection/Dynamic 1/Graphs\"  # Root directory containing all folders\n",
    "dataset = MalwareGraphDataset(root_directory, label_dict)\n",
    "\n",
    "# Count samples per class\n",
    "dataset.count_samples_per_class()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# Get indices of each class\n",
    "label_to_indices = {label: [] for label in label_dict.values()}\n",
    "\n",
    "for idx, data in enumerate(dataset):\n",
    "    label = data.y.item()  # Extract label\n",
    "    label_to_indices[label].append(idx)\n",
    "\n",
    "# Determine the minimum class size to balance all classes\n",
    "min_class_size = min(len(indices) for indices in label_to_indices.values())\n",
    "\n",
    "# Balance the dataset by sampling min_class_size samples from each class\n",
    "balanced_indices = []\n",
    "for indices in label_to_indices.values():\n",
    "    balanced_indices.extend(indices[:min_class_size])\n",
    "\n",
    "# Split balanced indices into 80% for training and 20% for testing\n",
    "train_indices, test_indices = train_test_split(\n",
    "    balanced_indices, test_size=0.2, stratify=[dataset[i].y.item() for i in balanced_indices], random_state=42\n",
    ")\n",
    "\n",
    "# Function to count samples per class\n",
    "def count_samples_per_class(indices, dataset, label_dict):\n",
    "    class_counts = defaultdict(int)\n",
    "    for idx in indices:\n",
    "        label = dataset[idx].y.item()\n",
    "        class_counts[label] += 1\n",
    "\n",
    "    print(\"Number of data points per class:\")\n",
    "    for label, count in class_counts.items():\n",
    "        class_name = list(label_dict.keys())[list(label_dict.values()).index(label)]\n",
    "        print(f\"{class_name} (Label {label}): {count} samples\")\n",
    "\n",
    "# Count samples per class in the training set\n",
    "print(\"Training Set (entire dataset):\")\n",
    "count_samples_per_class(train_indices, dataset, label_dict)\n",
    "\n",
    "# Count samples per class in the testing set (20% of dataset)\n",
    "print(\"\\nTesting Set (20% of dataset):\")\n",
    "count_samples_per_class(test_indices, dataset, label_dict)\n",
    "\n",
    "# Create train and test subsets\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample data point from the dataset\n",
    "sample_data = dataset[31]  # Access the first graph in your dataset\n",
    "\n",
    "# Get the input dimension by checking the number of columns in data.x\n",
    "input_dim = sample_data.x.size(1)\n",
    "print(f\"Input dimension (number of node features): {input_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = global_mean_pool(x, batch)  # Graph-level pooling\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 1  # Node feature size (adjust based on your actual feature size)\n",
    "hidden_dim = 64\n",
    "output_dim = 5  # Number of classes (benign, adware, etc.)\n",
    "\n",
    "model = GNNClassifier(input_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        \n",
    "        # Debugging: print batch sizes\n",
    "        print(f\"Output batch size: {out.size(0)}, Target batch size: {data.y.size(0)}\")\n",
    "        \n",
    "        if out.size(0) != data.y.size(0):\n",
    "            continue  # Skip mismatched batches as before\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    test_accuracy = test(model, test_loader, device)\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        labels = data.y.cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_dict.keys()))\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_model(model, train_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state_dict (recommended method)\n",
    "torch.save(model.state_dict(), \"gnn_model.pth\")\n",
    "print(\"Model saved as gnn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of the model\n",
    "model = GNNClassifier(input_dim=input_dim, hidden_dim=64, output_dim=5)\n",
    "\n",
    "# Load the saved state_dict into the model\n",
    "model.load_state_dict(torch.load(\"gnn_model.pth\"))\n",
    "print(\"Model loaded from gnn_model.pth\")\n",
    "\n",
    "# If you plan to use the model for inference, set it to evaluation mode\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MalwareEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
